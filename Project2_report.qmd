---
title: "Predicting beta-carotene content in plasma"
subtitle: "Project 2"
author: 
 - name: Zaide Montes
   affiliations:
     - name: Lund University
       department: Department of Biology
       address: Sölvegatan 12, S-223 62 Lund, Sweden
 - name: Dmytro Perepolkin
   affiliations:
     - name: Lund University
       department: Centre for Environmental and Climate Science
       address: Sölvegatan 37, S-223 62 Lund, Sweden
bibliography: Project2_plasma.bib
format: 
  titlepage-pdf:
    documentclass: scrbook
    classoption: ["oneside", "open=any"]
    number-sections: true
    titlepage: classic-lined 
    titlepage-logo: "img/logo.png"
    titlepage-footer: |
      Group 1
      Lund University\
      Lund, Sweden\
      [https://lu.se](https://lu.se)\
    titlepage-theme:
      elements: ["\\titleblock", "\\authorblock", "\\vfill", "\\logoblock", "\\footerblock"]
      page-align: "center"
      title-style: "doublelinewide"
      title-fontsize: 30
      title-fontstyle: "uppercase"
      title-space-after: "0.1\\textheight"
      subtitle-fontstyle: ["Large", "textit"]
      author-style: "plain"
      author-sep: "\\hskip1em"
      author-fontstyle: "Large"
      author-space-after: "2\\baselineskip"
      affiliation-style: "numbered-list-with-correspondence"
      affiliation-fontstyle: "large"
      affiliation-space-after: "0pt"
      footer-style: "plain"
      footer-fontstyle: ["large", "textsc"]
      footer-space-after: "0pt"
      logo-size: "0.25\\textheight"
      logo-space-after: "1cm"
fig-height: 5
fig-width: 7
fig-align: "center"
---

```{r setup, message=FALSE}
#| message: false
#install.packages("corrgram")
library(tidyverse)
library(broom)
library(corrgram) # visualisation of correlations
library(lmtest)  # more linear regression tools
library(hrbrthemes) #ggplot styling
library(GGally) # Pair plot
library(ggplot2) # ggplot 2
library(hrbrthemes) # theme for ggplot
library(kableExtra)
library(leaps)
library(glmnet)
library(patchwork)
library(plotmo)
library(randomForest) # for random forest chapter
library(pls) # for principal component regression
library(rsample) # for cross-validation
library(modelr) # evaluation of models
library(splines)
ggplot2::theme_set(theme_classic())

extrafont::loadfonts(device = "all", quiet = TRUE)

knitr::opts_chunk$set(dev = 'png')
options(device = function(file, width, height) {
  png(tempfile(), width = width, height = height)
})
```

# Introduction

## Data description

The dataset is from `gamlss.data` package [@harrell2002PlasmaRetinolBetaCarotene]. It is a cross-sectional study to investigate the relationship between personal characteristics and dietary factors, and plasma concentrations.

An original data frame has 315 observations of the following variables

| Variable        | Description                                             |
|------------------|------------------------------------------------------|
| `age`           | age(years)                                              |
| `sex`           | sex, 1=male, 2=female                                   |
| `smokstat`      | smoking status 1=never, 2=former, 3=current Smoker      |
| `bmi`           | body mass index weight/(height\^2)                      |
| `vituse`        | vitamin use 1=yes, fairly often, 2=yes, not often, 3=no |
| `calories`      | number of calories consumed per day                     |
| `fat`           | grams of fat consumed per day                           |
| `fiber`         | grams of fiber consumed per day                         |
| `alcohol`       | number of alcoholic drinks consumed per week            |
| `cholesterol`   | cholesterol consumed (mg per day)                       |
| `betadiet`      | dietary beta-carotene consumed (mcg per day)            |
| `retdiet`[^1]   | dietary retinol consumed (mcg per day)                  |
| `betaplasma`    | plasma beta-carotene (ng/ml)                            |
| `retplasma`[^2] | plasma retinol (ng/ml)                                  |

[^1]: Not present in the current version of the dataset

[^2]: Not present in the current version of the dataset

We import the data

```{r}
plasma_df <- read_csv("data/plasma.txt", show_col_types = FALSE)
```

> Observational studies have suggested that low dietary intake or low plasma concentrations of retinol, beta-carotene, or other carotenoids might be associated with increased risk of developing certain types of cancer... We designed a cross-sectional study to investigate the relationship between personal characteristics and dietary factors, and plasma concentrations of retinol, beta-carotene and other carotenoids. [@harrell2002PlasmaRetinolBetaCarotene]

# Multivariate regression

`glmnet` requires a model-matrix as the model specification. A model matrix is a matrix with a column for each predictor (i.e. $x$-variable) in the model (including the extra columns for polynomials, dummy variables etc), and each row is for a unique observation. Sometimes the model-matrix includes a column for the intercept, in `glmnet` it should (typically) not.

```{r}
#head(plasma_df)
x <- model.matrix(log(betaplasma) ~ ., plasma_df)[, -1]
#head(x)
y <- log(plasma_df$betaplasma)
#head(y)
```

## Ridge regression

```{r}
# Grid of lambdas
grid <- 10^seq(5, -2, length = 100) #grid 

# Run ridge regression (alpha =0) for each lambda in grid
ridge.mod <- glmnet(x, y, alpha = 0, lambda = grid)
```

The `ridge.mod` contains `r dim(coef(ridge.mod))[1]` rows and `r dim(coef(ridge.mod))[2]` columns corresponding to the number of $\lambda$ values we created earlier.

```{r}
# Prepare for cross validation by splitting the data in training and validation set
set.seed(42)
train <- sample(1:nrow(x), nrow(x) / 2)
test <- (-train)
y.test <- y[test]

```

```{r}
# Fit ridge regression on the training data, for our grid of lambda
ridge.mod <- glmnet(x[train, ], y[train], alpha = 0,
    lambda = grid, thresh = 1e-12)
# Find predictions for our testdata for a specific lambda (here lambda =4)
ridge.pred <- predict(ridge.mod, s = 4, newx = x[test, ])

```

The MSE for the $\lambda = 4$ is `r mean((ridge.pred - y.test)^2)`. @fig-ridge-plot1 shows the ridge model coefficients are approaching to zero.

```{r}
#| label: fig-ridge-plot1
#| out-width: 80%
#| fig-cap: The ridge regression coefficients are displayed for the Plasma data set, as a function of λ. 
#plot(ridge.mod,xvar="lambda",lwd=1.5)
plot_glmnet(ridge.mod,xvar="lambda",lwd=1.5)
```

```{r}
#| label: fig-ridge-plot2
#| fig-width: 10
#| fig-height: 5
#| out-width: 80%
#| fig-cap: A graph of the ridge model coefficients for different lambdas. The dashed lines are the log𝜆values corresponding to the 𝜆min (left dashed line) and 𝜆1𝑠𝑒(right dashed line).

set.seed(1)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 0)
plot(cv.out)
```

```{r}
# Choose the best:
bestlam <- cv.out$lambda.min
```

The best lambda is $\lambda=`r bestlam`$.

```{r}
#| label: fig-vaimp-ridge
#| fig-width: 10
#| fig-height: 5
#| out-width: 80%
#| fig-cap: Coefficients for the ridge model under best lambda

out <- glmnet(x, y, alpha = 0, lambda = grid)
ridge.coef <- predict(out, type = "coefficients",
    s = bestlam)

ridge.coef %>% as.matrix() %>% as.data.frame() %>%
  rownames_to_column()%>%
  arrange(-abs(s1)) %>% 
    filter(!str_detect(rowname, "Intercept")) %>% 
  ggplot()+geom_col(aes(x=s1,y=fct_reorder(rowname, abs(s1))))+
  labs(y="Coefficient", x="value")
```

```{r}
# MSE associated with this lambda
ridge.pred <- predict(ridge.mod, s = bestlam,
    newx = x[test, ])
ridge.test.mse <- mean((ridge.pred - y.test)^2)
```

The MSE turns out to be `r ridge.test.mse`.

## Lasso regression

```{r}
# Fit lasso model: alpha=1
# Use only the training data
lasso.mod <- glmnet(x[train, ], y[train], alpha = 1,
    lambda = grid)
```

@fig-Lasso-plot shows the coefficients against the penalty $\lambda$.

```{r}
#| label: fig-Lasso-plot
#| layout-ncol: 2
#| fig-width: 10
#| fig-height: 5
#| out-width: 80%
#| fig-cap: The Lasso regression coefficients 
#| fig-subcap: 
#|  - against df.
#|  - against lambda.
plot_glmnet(lasso.mod,label=TRUE,xvar="lambda",lwd=1.5)
plot_glmnet(lasso.mod,label=TRUE,lwd=1.5)
```

```{r}
#| label: fig-Lasso-plot3
#| fig-width: 10
#| fig-height: 5
#| out-width: 80%
#| fig-cap:  Cross-validated penalty for lasso model.
set.seed(1)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 1)
plot(cv.out)
```

The @fig-Lasso-plot3 illustrates the cross validation process for picking the best lambda in lasso regression. The dashed lines are the log $\lambda$ values corresponding to the $\lambda_{min}$ (left dashed line) and $\lambda_{1se}$ (right dashed line)

```{r}
bestlam2 <- cv.out$lambda.min
```

The best lambda value that minimizes the test MSE turns out to be $\lambda=`r bestlam2`$.

```{r}
# compute the test error for this choice of lambda or MSE associated with this lambda
lasso.pred <- predict(lasso.mod, s = bestlam2,
    newx = x[test, ])
lasso.test.mse <- mean((lasso.pred - y.test)^2)
```

The test MSE turns out to be `r lasso.test.mse`.

```{r}
#| label: fig-vaimp-lasso
#| fig-width: 10
#| fig-height: 5
#| out-width: 80%
#| fig-cap: Coefficients for the lasso model under best lambda

out <- glmnet(x, y, alpha = 1, lambda = grid)

lasso.coef <- predict(out, type = "coefficients",
    s = bestlam2)

lasso.coef %>% as.matrix() %>% as.data.frame() %>%
  rownames_to_column()%>%
  arrange(-abs(s1)) %>% 
  filter(!str_detect(rowname, "Intercept")) %>% 
  ggplot()+geom_col(aes(x=s1,y=fct_reorder(rowname, abs(s1))))+  labs(y="Coefficient", x="value")

```

The variance of ridge regression is slightly lower than the variance of the lasso. Consequently, the minimum MSE of ridge regression is slightly smaller than that of the lasso. In addition, the models generated from the lasso were much easier to interpret than those produced by ridge regression. The lasso yields sparse models that involve only a subset of the variables. Hence, depending on the value of $\lambda$, the lasso can produce a model involving reduced variables. In contrast, ridge regression will always include all of the variables in the model, although the magnitude of the coefficient estimates will depend on $\lambda$. In the lasso regression we observed that the variables that influenced more in the model are the `sex`, `vituse`, `smokestat`, `bmicat` (@fig-vaimp-lasso). To sum up, the lasso should perform better in a setting where a relatively small number of predictors have significant coefficients, and the remaining predictors have very small coefficients or that equal zero. In contrast, the ridge regression will perform better when the response is a function of many predictors, all with roughly equal-sized coefficients.

## Principle component regression

```{r}
pcr_mod <- pls::pcr(log(betaplasma)~., data=plasma_df, scale=TRUE, validation="CV")
#summary(pcr_mod)
```

@fig-pcr-validplot1 shows the out-of-fold (cross-validated) MSE for each number of components in the PCR.

```{r}
#| label: fig-pcr-validplot1
#| fig-width: 10
#| fig-height: 5
#| out-width: 80%
#| fig-cap:  Cross-validated MSE for PCR model.
validationplot(pcr_mod, val.type = "MSEP", main="")
```

We definitely dont need more then 8 components. Let's perform CV on the training dataset and see if we can reduce the model even further.

```{r}
#| label: fig-pcr-validplot2
#| fig-width: 10
#| fig-height: 5
#| out-width: 80%
#| fig-cap:  MSE for PCR model cross-validated on the training dataset.
set.seed(1)
pcr.fit <- pcr(log(betaplasma)~., data=plasma_df, ncomp=8, 
               subset = train, scale = TRUE, validation = "CV")
validationplot(pcr.fit, val.type = "MSEP",lwd=2,legendpos = "topright")
```

We select the lowest number of components which minimizes MSE

```{r}
pcr.pred <- predict(pcr.fit, x[test, ], ncomp = 3)
pcr.test.mse <- mean((pcr.pred - y.test)^2)
```

The test MSE for the PCR is `r pcr.test.mse`. 

# Random Forest

We will now use the random forest algorithm implemented in `randomForest` package in R.

The parameter `mtry` in random forest algorithm determines how many columns are consered for each split. We will have to cross-validate this parameter to make sure we are not overfitting to our dataset. We will now determine the best value of `mtry` parameter to use in our random forest model. @fig-rfcv shows the out-of-fold MSE for the models with different `mtry`.

```{r}
#| label: fig-rfcv
#| out-width: 80%
#| fig-cap: "Cross-validated tuning of parameter mtry"
set.seed(42)
tuneRF(x[train,], y[train], mtryStart = 3, 
       stepFactor=1.5, ntreeTry = 1000)
```

The @fig-rf-predicted-true1 shows the predicted values against the true values for the test portion of our dataset.

```{r}
rf_fit <- randomForest(log(betaplasma)~., data=plasma_df, mtry=3,
                       subset = train, importance=TRUE)

```


```{r}
#| label: fig-rf-predicted-true1
#| out-width: 80%
#| fig-cap: "Predicted vs true values for random forest algorithm"
rf_pred <- predict(rf_fit, newdata = plasma_df[test,])
plot(rf_pred, log(plasma_df$betaplasma)[test],
     ylab="log(betaplasma)", xlab="predicted")
abline(0,1)
rf.mse <- mean((rf_pred-y.test)^2)
```

The test MSE for random forest is `r rf.mse`, which is pretty impressive. As for any tree-based algorithm, interpretability is usually an issue. We can use the special function in the `randomForest` package to see which variables were most frequently used in the tree splits (@fig-rf-varimp1).

```{r}
#| label: fig-rf-varimp1
#| out-width: 80%
#| fig-cap: "Variable importance plot for random forest with mtry 12"
varimp_m <- importance(rf_fit) #matrix form
varImpPlot(rf_fit, main="")
```

It seems like `r glue::glue_collapse(names(sort(varimp_m[,1], decreasing = TRUE)[1:5]), sep=", ", last=", and ")` are the most important variables for predicting the level of beta-carotene in plasma.


# Non-linear effects

Now we are proceeding to developing a model for predicting the Greenhouse gas emissions using the non-linear effects, namely *splines*.

The Berkeley Earth data (Temp_BE.csv) provides the annual anomalies from the global mean temperature, calculated from the period Jan 1951-Dec 1980, which amounted to 14.105°C with 95% CI (14.080°C, 14.130°C).

```{r}
TEMPBE_df <- read_csv("data/Temp_BE.csv", show_col_types = FALSE) %>% 
  mutate(abs_temp=14.105+anomaly_1y) %>% 
  select(year, abs_temp)

GHG_df <- read_csv("data/GHG_1880_2014.csv", show_col_types = FALSE) %>% 
  rename(GHG=KYOTOGHG) %>% 
  mutate(GHG_cum=cumsum(replace_na(GHG, 0)))

GHG_TEMP_df <- left_join(GHG_df, TEMPBE_df, by="year")
```

```{r}
ns_mod <- lm(GHG_cum~ns(abs_temp, df=4), data=GHG_TEMP_df)
```

```{r}
lm_mod <- lm(GHG_cum~abs_temp, data=GHG_TEMP_df)
```

Compare spline to linear model in @fig-ghg-ns-lm

```{r}
#| label: fig-ghg-ns-lm
#| layout-ncol: 2
#| fig-width: 5
#| fig-height: 4
#| fig-cap: "Original data with the fitted values (including standard error)"
#| fig-subcap: 
#|   - "natural spline model"
#|   - "linear model"
predns_df <- predict(ns_mod, se = TRUE) %>% 
  as_tibble() %>% 
  bind_cols(GHG_TEMP_df)
predlm_df <- predict(lm_mod, se=TRUE) %>% 
  as_tibble() %>% 
  bind_cols(GHG_TEMP_df)

GHG_TEMP_df %>% ggplot(aes(x=abs_temp,y=GHG_cum))+ 
  geom_point(shape=21,alpha=0.5) +
  geom_line(data=predns_df,aes(y=fit),
            color="mediumpurple4",linewidth=1.5) + 
  geom_line(data=predns_df,aes(y=fit + 2*se.fit) , 
            color="mediumpurple4",linewidth=0.5,
            lty="dashed",alpha=0.8) + 
  geom_line(data=predns_df,aes(y=fit - 2*se.fit) , 
            color="mediumpurple4",linewidth=0.5,
            lty="dashed",alpha=0.8)+
  labs(x="Absolute temperature,°C", y="GHG emission, Gg CO2/yr")

GHG_TEMP_df %>% ggplot(aes(x=abs_temp,y=GHG_cum))+ 
  geom_point(shape=21,alpha=0.5) +
  geom_line(data=predlm_df,aes(y=fit),
            color="mediumpurple4",linewidth=1.5) + 
  geom_line(data=predlm_df,aes(y=fit + 2*se.fit) , 
            color="mediumpurple4",linewidth=0.5,
            lty="dashed",alpha=0.8) + 
  geom_line(data=predlm_df,aes(y=fit - 2*se.fit) , 
            color="mediumpurple4",linewidth=0.5,
            lty="dashed",alpha=0.8)+
  labs(x="Absolute temperature,°C", y="GHG emission, Gg CO2/yr")
```

We compare the fitted vs the actual values in @fig-fitactual-ns-lm.

```{r}
#| label: fig-fitactual-ns-lm
#| layout-ncol: 2
#| fig-width: 5
#| fig-height: 5
#| fig-cap: "Fitted vs actual values"
#| fig-subcap: 
#|   - "natural spline model"
#|   - "linear model"
predns_df %>% ggplot(aes(x=fit,y=GHG_cum))+ 
  geom_point(shape=21,alpha=0.5) +
  geom_abline(slope=1, intercept = 0)+
  labs(x="Fitted", y="Actual")

predlm_df %>% ggplot(aes(x=fit,y=GHG_cum))+ 
  geom_point(shape=21,alpha=0.5) +
  geom_abline(slope=1, intercept = 0)+
  labs(x="Fitted", y="Actual")
```

Now we tune the spline model changing the degrees of freedom and measuring the out-of-fold error. @fig-ns-cv-error shows MSE for ....

```{r}
#| label: fig-ns-cv-error
#| fig-width: 10
#| fig-height: 5
#| fig-cap: "Out-of-fold MSE for different values of df"
set.seed(42)
cv_error <- vector("numeric", length=10)
for(i in 1:10){
  ghg_glm_fit <- glm(GHG_cum~ns(abs_temp, df=i),
                     data=GHG_TEMP_df)
  cv_error[i] <- boot::cv.glm(GHG_TEMP_df, ghg_glm_fit, K=10)$delta[2]
}
#cv_error
plot(cv_error, type="l", xlab="df", ylab="MSE")
```

The first component is the raw cross-validation estimate of prediction error. The second component is the adjusted cross-validation estimate. The adjustment is designed to compensate for the bias introduced by not using leave-one-out cross-validation.


```{r}
ns_mod3 <- lm(GHG_cum~ns(abs_temp, df=3), data=GHG_TEMP_df)
ns_mod8 <- lm(GHG_cum~ns(abs_temp, df=8), data=GHG_TEMP_df)
```

Compare splines of different complexity @fig-ghg-ns38

```{r}
#| label: fig-ghg-ns38
#| layout-ncol: 2
#| fig-width: 10
#| fig-height: 5
#| fig-cap: "Original data with the fitted values (natural splines with df 3 and 8)"

predns3_df <- predict(ns_mod3, se = TRUE) %>% 
  as_tibble() %>% 
  bind_cols(GHG_TEMP_df)
predns8_df <- predict(ns_mod8, se = TRUE) %>% 
  as_tibble() %>% 
  bind_cols(GHG_TEMP_df)


GHG_TEMP_df %>% ggplot(aes(x=abs_temp,y=GHG_cum))+ 
  geom_point(shape=21,alpha=0.5) +
  geom_line(data=predns3_df,aes(y=fit),
            color="mediumpurple4",linewidth=1.5, alpha=0.5) + 
  geom_line(data=predns3_df,aes(y=fit + 2*se.fit) , 
            color="mediumpurple4",linewidth=0.5,
            lty="dashed",alpha=0.8) + 
  geom_line(data=predns3_df,aes(y=fit - 2*se.fit) , 
            color="mediumpurple4",linewidth=0.5,
            lty="dashed",alpha=0.8)+
    geom_line(data=predns8_df,aes(y=fit),
            color="firebrick",linewidth=1.5, alpha=0.5) + 
  geom_line(data=predns8_df,aes(y=fit + 2*se.fit) , 
            color="firebrick",linewidth=0.5,
            lty="dashed",alpha=0.8) + 
  geom_line(data=predns8_df,aes(y=fit - 2*se.fit) , 
            color="firebrick",linewidth=0.5,
            lty="dashed",alpha=0.8)+
  labs(x="Absolute temperature,°C", y="GHG emission, Gg CO2/yr")

```


# References
